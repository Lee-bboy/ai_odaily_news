#!/usr/bin/env python3
# -*- coding:utf-8 -*-
"""
æ”¹è¿›çš„æƒ…æ„Ÿæ ‡æ³¨è„šæœ¬
è§£å†³æ ‡æ³¨å€¾æ–œé—®é¢˜ï¼Œæé«˜æ ‡æ³¨è´¨é‡
"""

import pandas as pd
import jieba
import re
import logging
from database import DatabaseManager
import configparser
from typing import Dict, List, Tuple
import random

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ImprovedSentimentLabeler:
    """æ”¹è¿›çš„æƒ…æ„Ÿæ ‡æ³¨å™¨"""
    
    def __init__(self):
        # åŠ è½½æ”¹è¿›çš„æƒ…æ„Ÿè¯å…¸
        self.positive_words = self._load_improved_positive_words()
        self.negative_words = self._load_improved_negative_words()
        self.neutral_words = self._load_improved_neutral_words()
        
        # æƒ…æ„Ÿå¼ºåº¦è¯æ±‡
        self.intensity_words = self._load_intensity_words()
        
        # å¦å®šè¯å’Œè½¬æŠ˜è¯
        self.negation_words = ['ä¸', 'æ²¡', 'æ— ', 'é', 'æœª', 'åˆ«', 'è«', 'å‹¿', 'æ¯‹', 'å¼—', 'å¦', 'å', 'é€†', 'éš¾', 'å°‘', 'ä½']
        self.turn_words = ['ä½†æ˜¯', 'ç„¶è€Œ', 'ä¸è¿‡', 'å¯æ˜¯', 'åªæ˜¯', 'å´', 'åè€Œ', 'åå€’', 'åè€Œ', 'åå€’', 'è™½ç„¶', 'å°½ç®¡']
        
        # é‡‘èç‰¹å®šæ¨¡å¼
        self.financial_patterns = self._load_financial_patterns()
        
    def _load_improved_positive_words(self) -> set:
        """åŠ è½½æ”¹è¿›çš„æ­£é¢è¯æ±‡"""
        words = {
            # åŸºç¡€æ­£é¢è¯æ±‡
            'ä¸Šæ¶¨', 'ä¸Šå‡', 'èµ°é«˜', 'æ”€å‡', 'ä¸Šæ‰¬', 'å¢é•¿', 'å¢åŠ ', 'æé«˜', 'æå‡', 'æ”¹å–„',
            'åˆ©å¥½', 'å¥½æ¶ˆæ¯', 'ç§¯æ', 'æ­£é¢', 'ä¹è§‚', 'å¼ºåŠ²', 'ç¹è£', 'å…´æ—º', 'å‘è¾¾',
            'æˆåŠŸ', 'èƒœåˆ©', 'çªç ´', 'åˆ›æ–°', 'é¢†å…ˆ', 'ä¼˜ç§€', 'å“è¶Š', 'æ°å‡º', 'çªå‡º',
            'ç¨³å®š', 'å®‰å…¨', 'å¯é ', 'ä¿¡ä»»', 'ä¿¡å¿ƒ', 'å¸Œæœ›', 'æœºé‡', 'æœºä¼š', 'å‰æ™¯',
            'ç›ˆåˆ©', 'æ”¶ç›Š', 'å›æŠ¥', 'åˆ©æ¶¦', 'æ”¶å…¥', 'é”€å”®é¢', 'å¸‚åœºä»½é¢', 'ç«äº‰åŠ›',
            
            # é‡‘èç‰¹å®šæ­£é¢è¯æ±‡
            'ç‰›å¸‚', 'æ¶¨åœ', 'å¤§æ¶¨', 'æš´æ¶¨', 'é£™å‡', 'çªç ´', 'æ–°é«˜', 'å†å²æ–°é«˜', 'åˆ›çºªå½•',
            'ä¸šç»©', 'ç›ˆåˆ©', 'å‡€åˆ©æ¶¦', 'è¥æ”¶', 'é”€å”®é¢', 'å¸‚åœºä»½é¢', 'è¡Œä¸šé¾™å¤´', 'é¾™å¤´è‚¡',
            'æ”¿ç­–æ”¯æŒ', 'å‡ç¨é™è´¹', 'é™æ¯', 'é™å‡†', 'æµåŠ¨æ€§', 'èµ„é‡‘é¢', 'æŠ•èµ„æœºä¼š',
            'å¹¶è´­', 'é‡ç»„', 'ä¸Šå¸‚', 'IPO', 'èèµ„', 'æŠ•èµ„', 'æ‰©å¼ ', 'å‘å±•', 'å¢é•¿',
            
            # å¸‚åœºæƒ…ç»ªè¯æ±‡
            'çœ‹å¥½', 'ä¹è§‚', 'ä¿¡å¿ƒ', 'çƒ­æƒ…', 'è¿½æ§', 'æŠ¢è´­', 'ç«çˆ†', 'çƒ­é”€', 'ç•…é”€',
            'é¢†å…ˆ', 'ç¬¬ä¸€', 'æœ€ä½³', 'ä¼˜è´¨', 'ä¼˜ç§€', 'å“è¶Š', 'çªå‡º', 'æ˜¾è‘—', 'æ˜æ˜¾',
            'å¤§å¹…', 'æ˜¾è‘—', 'æ˜æ˜¾', 'å¼ºåŠ²', 'æœ‰åŠ›', 'æœ‰æ•ˆ', 'æˆåŠŸ', 'é¡ºåˆ©', 'åœ†æ»¡'
        }
        return words
    
    def _load_improved_negative_words(self) -> set:
        """åŠ è½½æ”¹è¿›çš„è´Ÿé¢è¯æ±‡"""
        words = {
            # åŸºç¡€è´Ÿé¢è¯æ±‡
            'ä¸‹è·Œ', 'ä¸‹é™', 'èµ°ä½', 'ä¸‹æŒ«', 'å›è½', 'å‡å°‘', 'é™ä½', 'æ¶åŒ–', 'è¡°é€€', 'èç¼©',
            'åˆ©ç©º', 'åæ¶ˆæ¯', 'æ¶ˆæ', 'è´Ÿé¢', 'æ‚²è§‚', 'ç–²è½¯', 'ä½è¿·', 'è§æ¡', 'å›°éš¾',
            'å¤±è´¥', 'æŸå¤±', 'äºæŸ', 'å€ºåŠ¡', 'é£é™©', 'å±æœº', 'é—®é¢˜', 'å›°éš¾', 'æŒ‘æˆ˜',
            'ä¸ç¨³å®š', 'ä¸å®‰å…¨', 'ä¸å¯é ', 'ä¸ä¿¡ä»»', 'å¤±æœ›', 'æ‹…å¿§', 'ææƒ§', 'ææ…Œ',
            'äºæŸ', 'æŸå¤±', 'å€ºåŠ¡', 'ç ´äº§', 'å€’é—­', 'è£å‘˜', 'å¤±ä¸š', 'ç»æµå±æœº',
            
            # é‡‘èç‰¹å®šè´Ÿé¢è¯æ±‡
            'ç†Šå¸‚', 'è·Œåœ', 'å¤§è·Œ', 'æš´è·Œ', 'è·³æ°´', 'ç ´ä½', 'æ–°ä½', 'å†å²æ–°ä½', 'åˆ›æ–°ä½',
            'ä¸šç»©ä¸‹æ»‘', 'äºæŸ', 'å‡€åˆ©æ¶¦ä¸‹é™', 'è¥æ”¶ä¸‹é™', 'é”€å”®é¢ä¸‹é™', 'å¸‚åœºä»½é¢ä¸‹é™',
            'æ”¿ç­–æ”¶ç´§', 'åŠ æ¯', 'åŠ å‡†', 'æµåŠ¨æ€§ç´§å¼ ', 'èµ„é‡‘é¢ç´§å¼ ', 'æŠ•èµ„é£é™©',
            'é€€å¸‚', 'ç ´äº§', 'å€ºåŠ¡è¿çº¦', 'ä¿¡ç”¨é£é™©', 'å¸‚åœºé£é™©', 'ç³»ç»Ÿæ€§é£é™©',
            
            # å¸‚åœºæƒ…ç»ªè¯æ±‡
            'çœ‹ç©º', 'æ‚²è§‚', 'æ‹…å¿§', 'ææ…Œ', 'æŠ›å”®', 'æ¸…ä»“', 'å‰²è‚‰', 'æ­¢æŸ', 'å¥—ç‰¢',
            'è½å', 'å«åº•', 'æœ€å·®', 'åŠ£è´¨', 'ç³Ÿç³•', 'æ¶åŠ£', 'ä¸¥é‡', 'é‡å¤§', 'å·¨å¤§',
            'å¤§å¹…', 'æ˜¾è‘—', 'æ˜æ˜¾', 'ç–²è½¯', 'æ— åŠ›', 'æ— æ•ˆ', 'å¤±è´¥', 'å›°éš¾', 'æŒ«æŠ˜'
        }
        return words
    
    def _load_improved_neutral_words(self) -> set:
        """åŠ è½½æ”¹è¿›çš„ä¸­æ€§è¯æ±‡"""
        words = {
            # åŸºç¡€ä¸­æ€§è¯æ±‡
            'å¹³ç¨³', 'ç¨³å®š', 'ç»´æŒ', 'ä¿æŒ', 'è§‚å¯Ÿ', 'åˆ†æ', 'ç ”ç©¶', 'è°ƒæŸ¥', 'ç»Ÿè®¡',
            'å‘å¸ƒ', 'å…¬å¸ƒ', 'å®£å¸ƒ', 'é€šçŸ¥', 'æŠ¥å‘Š', 'æ•°æ®', 'æŒ‡æ ‡', 'è¶‹åŠ¿', 'å˜åŒ–',
            'æ”¿ç­–', 'è§„å®š', 'åˆ¶åº¦', 'æ ‡å‡†', 'è¦æ±‚', 'å»ºè®®', 'æ„è§', 'çœ‹æ³•', 'è§‚ç‚¹',
            'ä¼šè®®', 'è®¨è®º', 'åå•†', 'åˆä½œ', 'äº¤æµ', 'æ²Ÿé€š', 'è”ç³»', 'å…³ç³»', 'å½±å“',
            
            # æ—¶é—´è¯æ±‡
            'ä»Šæ—¥', 'æ˜¨æ—¥', 'æœ¬å‘¨', 'æœ¬æœˆ', 'ä»Šå¹´', 'è¿‘æœŸ', 'æœªæ¥', 'é¢„æœŸ', 'é¢„è®¡',
            'å³å°†', 'å°†è¦', 'è®¡åˆ’', 'å®‰æ’', 'å‡†å¤‡', 'è€ƒè™‘', 'ç ”ç©¶', 'æ¢è®¨', 'è®¨è®º',
            
            # ç¨‹åº¦è¯æ±‡
            'ä¸€èˆ¬', 'æ™®é€š', 'æ­£å¸¸', 'å¸¸è§„', 'æ ‡å‡†', 'å¹³å‡', 'ä¸­ç­‰', 'é€‚ä¸­', 'é€‚åº¦'
        }
        return words
    
    def _load_intensity_words(self) -> Dict[str, float]:
        """åŠ è½½æƒ…æ„Ÿå¼ºåº¦è¯æ±‡"""
        intensity = {
            # æå¼ºæ­£é¢
            'æš´æ¶¨': 3.0, 'é£™å‡': 3.0, 'çªç ´': 2.5, 'å†å²æ–°é«˜': 2.5, 'æ¶¨åœ': 2.5,
            'æå¤§åˆ©å¥½': 3.0, 'é‡å¤§çªç ´': 2.5, 'æ˜¾è‘—æ”¹å–„': 2.0, 'å¤§å¹…å¢é•¿': 2.0,
            'åˆ›çºªå½•': 2.5, 'å‰æ‰€æœªæœ‰': 2.5, 'å²æ— å‰ä¾‹': 2.5, 'é‡Œç¨‹ç¢‘': 2.0,
            
            # å¼ºæ­£é¢
            'å¤§æ¶¨': 2.0, 'ä¸Šæ¶¨': 1.5, 'å¢é•¿': 1.5, 'æ”¹å–„': 1.5, 'åˆ©å¥½': 1.8,
            'ç§¯æ': 1.5, 'ä¹è§‚': 1.5, 'å¼ºåŠ²': 1.8, 'ä¼˜ç§€': 1.5, 'æˆåŠŸ': 1.8,
            'çœ‹å¥½': 1.8, 'ä¿¡å¿ƒ': 1.5, 'çƒ­æƒ…': 1.5, 'è¿½æ§': 1.8,
            
            # ä¸­ç­‰æ­£é¢
            'å°å¹…ä¸Šæ¶¨': 1.0, 'å¾®æ¶¨': 0.8, 'ç¨³å®š': 0.5, 'ç»´æŒ': 0.5, 'å¹³ç¨³': 0.5,
            
            # æå¼ºè´Ÿé¢
            'æš´è·Œ': -3.0, 'è·³æ°´': -3.0, 'ç ´ä½': -2.5, 'å†å²æ–°ä½': -2.5, 'è·Œåœ': -2.5,
            'æå¤§åˆ©ç©º': -3.0, 'é‡å¤§å±æœº': -2.5, 'æ˜¾è‘—æ¶åŒ–': -2.0, 'å¤§å¹…ä¸‹é™': -2.0,
            'åˆ›æ–°ä½': -2.5, 'å‰æ‰€æœªæœ‰': -2.5, 'å²æ— å‰ä¾‹': -2.5, 'ç¾éš¾æ€§': -2.5,
            
            # å¼ºè´Ÿé¢
            'å¤§è·Œ': -2.0, 'ä¸‹è·Œ': -1.5, 'ä¸‹é™': -1.5, 'æ¶åŒ–': -1.5, 'åˆ©ç©º': -1.8,
            'æ¶ˆæ': -1.5, 'æ‚²è§‚': -1.5, 'ç–²è½¯': -1.8, 'å¤±è´¥': -1.8, 'äºæŸ': -1.8,
            'çœ‹ç©º': -1.8, 'æ‹…å¿§': -1.5, 'ææ…Œ': -1.8, 'æŠ›å”®': -1.8,
            
            # ä¸­ç­‰è´Ÿé¢
            'å°å¹…ä¸‹è·Œ': -1.0, 'å¾®è·Œ': -0.8, 'è°ƒæ•´': -0.5, 'æ³¢åŠ¨': -0.3
        }
        return intensity
    
    def _load_financial_patterns(self) -> Dict[str, float]:
        """åŠ è½½é‡‘èæ¨¡å¼åŒ¹é…"""
        patterns = {
            # æ­£é¢æ¨¡å¼
            r'ä¸Šæ¶¨\s*\d+%': 2.0,  # ä¸Šæ¶¨X%
            r'å¢é•¿\s*\d+%': 1.8,  # å¢é•¿X%
            r'çªç ´\s*\d+': 2.0,   # çªç ´X
            r'åˆ›\s*æ–°é«˜': 2.5,     # åˆ›æ–°é«˜
            r'åˆ©å¥½': 1.8,          # åˆ©å¥½
            r'æ”¯æŒ': 1.5,          # æ”¯æŒ
            r'ä¿ƒè¿›': 1.5,          # ä¿ƒè¿›
            
            # è´Ÿé¢æ¨¡å¼
            r'ä¸‹è·Œ\s*\d+%': -2.0, # ä¸‹è·ŒX%
            r'ä¸‹é™\s*\d+%': -1.8, # ä¸‹é™X%
            r'è·Œç ´\s*\d+': -2.0,  # è·Œç ´X
            r'åˆ›\s*æ–°ä½': -2.5,   # åˆ›æ–°ä½
            r'åˆ©ç©º': -1.8,         # åˆ©ç©º
            r'æ”¶ç´§': -1.5,         # æ”¶ç´§
            r'é™åˆ¶': -1.5,         # é™åˆ¶
        }
        return patterns
    
    def analyze_sentiment_improved(self, title: str, description: str) -> Tuple[str, float]:
        """æ”¹è¿›çš„æƒ…æ„Ÿåˆ†æ"""
        if pd.isna(title):
            title = ""
        if pd.isna(description):
            description = ""
        
        # ç»„åˆæ ‡é¢˜å’Œæè¿°
        combined_text = f"{title} {description}".strip()
        if not combined_text:
            return 'neutral', 0.5
        
        # åˆ†è¯
        words = list(jieba.cut(combined_text))
        
        # è®¡ç®—æƒ…æ„Ÿåˆ†æ•°
        sentiment_score = 0.0
        word_count = len(words)
        
        # æ£€æŸ¥å¦å®šè¯å’Œè½¬æŠ˜è¯
        has_negation = any(word in combined_text for word in self.negation_words)
        has_turn = any(word in combined_text for word in self.turn_words)
        
        # åˆ†ææ¯ä¸ªè¯çš„æƒ…æ„Ÿ
        for word in words:
            word_score = 0.0
            
            # æ£€æŸ¥æƒ…æ„Ÿå¼ºåº¦è¯æ±‡
            if word in self.intensity_words:
                word_score = self.intensity_words[word]
            # æ£€æŸ¥æ­£é¢è¯æ±‡
            elif word in self.positive_words:
                word_score = 1.0
            # æ£€æŸ¥è´Ÿé¢è¯æ±‡
            elif word in self.negative_words:
                word_score = -1.0
            # æ£€æŸ¥ä¸­æ€§è¯æ±‡
            elif word in self.neutral_words:
                word_score = 0.0
            
            sentiment_score += word_score
        
        # æ¨¡å¼åŒ¹é…
        for pattern, score in self.financial_patterns.items():
            if re.search(pattern, combined_text):
                sentiment_score += score
        
        # å¤„ç†å¦å®šè¯
        if has_negation:
            sentiment_score = -sentiment_score * 0.8
        
        # å¤„ç†è½¬æŠ˜è¯
        if has_turn:
            sentiment_score = sentiment_score * 0.6
        
        # æ ‡å‡†åŒ–åˆ†æ•°
        if word_count > 0:
            normalized_score = sentiment_score / word_count
        else:
            normalized_score = 0.0
        
        # è°ƒæ•´é˜ˆå€¼ï¼Œé™ä½neutralçš„æ¯”ä¾‹
        if normalized_score > 0.05:  # é™ä½æ­£é¢é˜ˆå€¼
            sentiment = 'positive'
        elif normalized_score < -0.05:  # é™ä½è´Ÿé¢é˜ˆå€¼
            sentiment = 'negative'
        else:
            sentiment = 'neutral'
        
        # è®¡ç®—ç½®ä¿¡åº¦
        confidence = min(abs(normalized_score) * 3, 0.95)
        confidence = max(confidence, 0.6)  # æé«˜æœ€ä½ç½®ä¿¡åº¦
        
        return sentiment, confidence
    
    def batch_analyze_improved(self, data: List[Dict]) -> List[Dict]:
        """æ‰¹é‡åˆ†ææƒ…æ„Ÿï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
        results = []
        
        for i, item in enumerate(data):
            title = item.get('title', '')
            description = item.get('description', '')
            
            sentiment, confidence = self.analyze_sentiment_improved(title, description)
            
            result = item.copy()
            result['predicted_sentiment'] = sentiment
            result['confidence'] = confidence
            
            results.append(result)
            
            if (i + 1) % 100 == 0:
                logger.info(f"å·²å¤„ç† {i + 1}/{len(data)} æ¡æ•°æ®")
        
        return results

def rebalance_sentiment_labels():
    """é‡æ–°å¹³è¡¡æƒ…æ„Ÿæ ‡ç­¾"""
    
    try:
        logger.info("å¼€å§‹é‡æ–°å¹³è¡¡æƒ…æ„Ÿæ ‡ç­¾...")
        
        # è¿æ¥æ•°æ®åº“
        db_manager = DatabaseManager()
        
        # è·å–å·²æ ‡æ³¨æ•°æ®
        labeled_data = db_manager.get_labeled_data()
        
        if labeled_data.empty:
            logger.info("æ²¡æœ‰æ‰¾åˆ°å·²æ ‡æ³¨æ•°æ®")
            return False
        
        logger.info(f"æ‰¾åˆ° {len(labeled_data)} æ¡å·²æ ‡æ³¨æ•°æ®")
        
        # åˆ›å»ºæ”¹è¿›çš„æ ‡æ³¨å™¨
        labeler = ImprovedSentimentLabeler()
        
        # é‡æ–°åˆ†ææƒ…æ„Ÿ
        logger.info("é‡æ–°åˆ†ææƒ…æ„Ÿ...")
        
        # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
        data_list = []
        for _, row in labeled_data.iterrows():
            data_list.append({
                'id': getattr(row, db_manager.id_column),
                'title': getattr(row, db_manager.title_column),
                'description': getattr(row, db_manager.description_column)
            })
        
        # é‡æ–°æ ‡æ³¨
        relabeled_results = labeler.batch_analyze_improved(data_list)
        
        # ç»Ÿè®¡æ ‡æ³¨ç»“æœ
        sentiment_counts = {}
        for result in relabeled_results:
            sentiment = result['predicted_sentiment']
            sentiment_counts[sentiment] = sentiment_counts.get(sentiment, 0) + 1
        
        logger.info("é‡æ–°æ ‡æ³¨ç»“æœç»Ÿè®¡:")
        for sentiment, count in sentiment_counts.items():
            logger.info(f"  {sentiment}: {count}")
        
        # æ›´æ–°æ•°æ®åº“
        logger.info("å¼€å§‹æ›´æ–°æ•°æ®åº“...")
        
        updates = []
        for result in relabeled_results:
            updates.append({
                'id': result['id'],
                'sentiment': result['predicted_sentiment'],
                'confidence': result['confidence']
            })
        
        # æ‰¹é‡æ›´æ–°
        db_manager.batch_update_sentiment(updates)
        
        logger.info(f"âœ… æˆåŠŸé‡æ–°æ ‡æ³¨ {len(updates)} æ¡æ•°æ®")
        
        return True
        
    except Exception as e:
        logger.error(f"é‡æ–°å¹³è¡¡æƒ…æ„Ÿæ ‡ç­¾å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("æ”¹è¿›çš„æƒ…æ„Ÿæ ‡æ³¨ç³»ç»Ÿ")
    print("=" * 60)
    
    print("\né€‰æ‹©æ“ä½œ:")
    print("1. é‡æ–°å¹³è¡¡ç°æœ‰æ ‡æ³¨æ•°æ®")
    print("2. é€€å‡º")
    
    while True:
        choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-2): ").strip()
        
        if choice == '1':
            print("\nå¼€å§‹é‡æ–°å¹³è¡¡æƒ…æ„Ÿæ ‡ç­¾...")
            success = rebalance_sentiment_labels()
            
            if success:
                print("\nğŸ‰ æƒ…æ„Ÿæ ‡ç­¾é‡æ–°å¹³è¡¡å®Œæˆï¼")
                print("ç°åœ¨å¯ä»¥è¿è¡Œè®­ç»ƒè„šæœ¬:")
                print("python3 enhanced_train_model.py")
            else:
                print("\nâŒ é‡æ–°å¹³è¡¡å¤±è´¥")
            break
            
        elif choice == '2':
            print("é€€å‡ºç¨‹åº")
            break
            
        else:
            print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

if __name__ == "__main__":
    main()
