#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€Ÿä¿®å¤æ•°æ®é—®é¢˜çš„è„šæœ¬
è§£å†³æ ·æœ¬æ•°é‡ä¸è¶³å¯¼è‡´çš„è®­ç»ƒé—®é¢˜
"""

import pandas as pd
import numpy as np
from data_augmentation import DataAugmentor
from data_processor import TextProcessor, DatasetBuilder
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def create_sample_data():
    """åˆ›å»ºè¶³å¤Ÿçš„æ ·æœ¬æ•°æ®ç”¨äºæµ‹è¯•"""
    print("åˆ›å»ºæ ·æœ¬æ•°æ®...")
    
    # åŸºç¡€æ ·æœ¬
    base_data = [
        # positive samples
        ('è‚¡ç¥¨å¤§æ¶¨', 'ä»Šæ—¥è‚¡å¸‚è¡¨ç°å¼ºåŠ²ï¼Œä¸»è¦æŒ‡æ•°ä¸Šæ¶¨è¶…è¿‡2%', 'positive'),
        ('æ”¿ç­–åˆ©å¥½', 'æ”¿åºœå‡ºå°æ–°æ”¿ç­–ï¼Œæ”¯æŒç»æµå‘å±•', 'positive'),
        ('ç»æµæ•°æ®å‘å¥½', 'æœ€æ–°ç»æµæ•°æ®æ˜¾ç¤ºå¢é•¿æ€åŠ¿è‰¯å¥½', 'positive'),
        ('å¤®è¡Œé™æ¯', 'å¤®è¡Œå®£å¸ƒé™æ¯ï¼Œé‡Šæ”¾æµåŠ¨æ€§', 'positive'),
        ('æŠ•èµ„æœºä¼š', 'å¸‚åœºå‡ºç°æ–°çš„æŠ•èµ„æœºä¼š', 'positive'),
        ('ä¼ä¸šç›ˆåˆ©', 'å¤šå®¶ä¼ä¸šå‘å¸ƒç›ˆåˆ©æŠ¥å‘Šï¼Œä¸šç»©è¶…é¢„æœŸ', 'positive'),
        ('å¸‚åœºä¿¡å¿ƒ', 'æŠ•èµ„è€…ä¿¡å¿ƒå¢å¼ºï¼Œå¸‚åœºæƒ…ç»ªä¹è§‚', 'positive'),
        ('æŠ€æœ¯çªç ´', 'æ–°æŠ€æœ¯å–å¾—é‡å¤§çªç ´ï¼Œæ¨åŠ¨è¡Œä¸šå‘å±•', 'positive'),
        
        # negative samples
        ('å¸‚åœºä¸‹è·Œ', 'å—å¤–éƒ¨å› ç´ å½±å“ï¼Œå¸‚åœºå‡ºç°è°ƒæ•´', 'negative'),
        ('ç»æµä¸‹è¡Œ', 'ç»æµæ•°æ®æ˜¾ç¤ºä¸‹è¡Œå‹åŠ›å¢å¤§', 'negative'),
        ('æ”¿ç­–æ”¶ç´§', 'ç›‘ç®¡æ”¿ç­–æ”¶ç´§ï¼Œå¸‚åœºæµåŠ¨æ€§å‡å°‘', 'negative'),
        ('ä¼ä¸šäºæŸ', 'å¤šå®¶ä¼ä¸šå‘å¸ƒäºæŸæŠ¥å‘Šï¼Œä¸šç»©ä¸åŠé¢„æœŸ', 'negative'),
        ('å¸‚åœºææ…Œ', 'æŠ•èµ„è€…ææ…Œæƒ…ç»ªè”“å»¶ï¼Œå¸‚åœºæ³¢åŠ¨åŠ å‰§', 'negative'),
        ('é£é™©äº‹ä»¶', 'çªå‘é£é™©äº‹ä»¶å½±å“å¸‚åœºç¨³å®š', 'negative'),
        ('ç›‘ç®¡å¤„ç½š', 'å¤šå®¶æœºæ„å› è¿è§„è¢«ç›‘ç®¡å¤„ç½š', 'negative'),
        ('å¸‚åœºä½è¿·', 'å¸‚åœºäº¤æŠ•æ¸…æ·¡ï¼Œæˆäº¤é‡èç¼©', 'negative'),
        
        # neutral samples
        ('å¸‚åœºè§‚å¯Ÿ', 'å¸‚åœºè¡¨ç°å¹³ç¨³ï¼ŒæŠ•èµ„è€…è§‚æœ›æƒ…ç»ªæµ“åš', 'neutral'),
        ('æ”¿ç­–è§£è¯»', 'ä¸“å®¶è§£è¯»æœ€æ–°æ”¿ç­–å¯¹å¸‚åœºçš„å½±å“', 'neutral'),
        ('æ•°æ®å‘å¸ƒ', 'ç»Ÿè®¡å±€å‘å¸ƒæœ€æ–°ç»æµæ•°æ®', 'neutral'),
        ('ä¼šè®®å¬å¼€', 'é‡è¦ç»æµä¼šè®®å³å°†å¬å¼€', 'neutral'),
        ('è¡Œä¸šåˆ†æ', 'åˆ†æå¸ˆå‘å¸ƒè¡Œä¸šç ”ç©¶æŠ¥å‘Š', 'neutral'),
        ('å¸‚åœºåŠ¨æ€', 'å¸‚åœºå‡ºç°æ–°çš„å˜åŒ–å’Œè¶‹åŠ¿', 'neutral'),
        ('æ”¿ç­–é¢„æœŸ', 'å¸‚åœºå¯¹å³å°†å‡ºå°çš„æ”¿ç­–æœ‰æ‰€é¢„æœŸ', 'neutral'),
        ('æŠ€æœ¯å‘å±•', 'æ–°æŠ€æœ¯åœ¨è¡Œä¸šä¸­çš„åº”ç”¨æƒ…å†µ', 'neutral')
    ]
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(base_data, columns=['title', 'description', 'sentiment'])
    
    print(f"åˆ›å»ºäº† {len(df)} ä¸ªåŸºç¡€æ ·æœ¬")
    print("æ ‡ç­¾åˆ†å¸ƒ:")
    print(df['sentiment'].value_counts())
    
    return df

def enhance_dataset(df, target_samples_per_class=15):
    """å¢å¼ºæ•°æ®é›†"""
    print(f"\nå¼€å§‹æ•°æ®å¢å¼ºï¼Œç›®æ ‡æ¯ç±» {target_samples_per_class} ä¸ªæ ·æœ¬...")
    
    augmentor = DataAugmentor()
    enhanced_df = augmentor.create_balanced_dataset(
        df, 'title', 'description', 'sentiment', 
        samples_per_class=target_samples_per_class
    )
    
    return enhanced_df

def test_data_processing(enhanced_df):
    """æµ‹è¯•æ•°æ®å¤„ç†"""
    print("\næµ‹è¯•æ•°æ®å¤„ç†...")
    
    processor = TextProcessor()
    builder = DatasetBuilder(processor)
    
    # å‡†å¤‡æ•°æ®é›†
    texts, labels = builder.prepare_dataset(
        enhanced_df, 'title', 'description', 'sentiment'
    )
    
    print(f"å¤„ç†åçš„æ–‡æœ¬æ•°é‡: {len(texts)}")
    print(f"æ ‡ç­¾æ•°é‡: {len(labels)}")
    
    if len(texts) > 0:
        try:
            # æµ‹è¯•æ•°æ®é›†åˆ†å‰²
            train_data, val_data, test_data = builder.split_dataset(texts, labels)
            print("âœ“ æ•°æ®é›†åˆ†å‰²æˆåŠŸ")
            print(f"  è®­ç»ƒé›†: {len(train_data[0])} æ ·æœ¬")
            print(f"  éªŒè¯é›†: {len(val_data[0])} æ ·æœ¬")
            print(f"  æµ‹è¯•é›†: {len(test_data[0])} æ ·æœ¬")
            
            return True
        except Exception as e:
            print(f"âœ— æ•°æ®é›†åˆ†å‰²å¤±è´¥: {e}")
            return False
    
    return False

def save_enhanced_data(enhanced_df, filename='enhanced_sample_data.csv'):
    """ä¿å­˜å¢å¼ºåçš„æ•°æ®"""
    enhanced_df.to_csv(filename, index=False, encoding='utf-8')
    print(f"\nå¢å¼ºåçš„æ•°æ®å·²ä¿å­˜åˆ°: {filename}")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    print("\næœ€ç»ˆæ•°æ®ç»Ÿè®¡:")
    print(f"æ€»æ ·æœ¬æ•°: {len(enhanced_df)}")
    print("æ ‡ç­¾åˆ†å¸ƒ:")
    print(enhanced_df['sentiment'].value_counts())
    
    # æ˜¾ç¤ºå¢å¼ºæ ·æœ¬ç»Ÿè®¡
    if 'is_augmented' in enhanced_df.columns:
        augmented_count = enhanced_df['is_augmented'].sum()
        original_count = len(enhanced_df) - augmented_count
        print(f"\nåŸå§‹æ ·æœ¬: {original_count}")
        print(f"å¢å¼ºæ ·æœ¬: {augmented_count}")
        
        if augmented_count > 0:
            print("\nå¢å¼ºæ–¹æ³•ç»Ÿè®¡:")
            method_counts = enhanced_df[enhanced_df['is_augmented']]['augmentation_method'].value_counts()
            print(method_counts)

def main():
    """ä¸»å‡½æ•°"""
    print("è‚¡ç¥¨æ¶ˆæ¯æƒ…æ„Ÿåˆ†æç³»ç»Ÿ - æ•°æ®é—®é¢˜ä¿®å¤è„šæœ¬")
    print("=" * 60)
    
    try:
        # 1. åˆ›å»ºåŸºç¡€æ ·æœ¬æ•°æ®
        base_df = create_sample_data()
        
        # 2. å¢å¼ºæ•°æ®é›†
        enhanced_df = enhance_dataset(base_df, target_samples_per_class=20)
        
        # 3. æµ‹è¯•æ•°æ®å¤„ç†
        success = test_data_processing(enhanced_df)
        
        if success:
            print("\nğŸ‰ æ•°æ®å¤„ç†æµ‹è¯•æˆåŠŸï¼")
            
            # 4. ä¿å­˜å¢å¼ºåçš„æ•°æ®
            save_enhanced_data(enhanced_df)
            
            print("\nç°åœ¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è¿›è¡Œè®­ç»ƒ:")
            print("python3 enhanced_train_model.py")
            
        else:
            print("\nâš ï¸  æ•°æ®å¤„ç†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç ")
            
    except Exception as e:
        print(f"\nâŒ è„šæœ¬æ‰§è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
